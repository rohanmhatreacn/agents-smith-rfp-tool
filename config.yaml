# Configuration file for AI RFP Assistant

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Model provider: "ollama" for local, "bedrock" for AWS
MODEL_PROVIDER=ollama

# Model ID for Ollama (e.g., "qwen3:4b", "llama3.2:latest")
MODEL_ID=qwen3:4b

# Model temperature (0.0 to 1.0)
TEMPERATURE=0.3

# Ollama host URL
OLLAMA_HOST=http://localhost:11434

# AWS Bedrock configuration (if using Bedrock)
AWS_REGION=us-west-2
BEDROCK_MODEL_ID=us.anthropic.claude-3-sonnet-20240229-v1:0

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Use local services for development (true) or AWS services for production (false)
DYNAMODB_LOCAL=true
S3_LOCAL=true

# DynamoDB configuration
DYNAMODB_TABLE_NAME=rfp-assistant-sessions

# S3/MinIO configuration
S3_BUCKET_NAME=rfp-assistant-documents

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================

# Chainlit host and port
CHAINLIT_HOST=0.0.0.0
CHAINLIT_PORT=8080

# MCP mode: "local" for local processing, "cloud" for AWS-hosted MCP servers
MCP_MODE=local

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Authentication credentials (for development)
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO